{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1tvgSHNDp2UEqiJ-xq-YXP89rj4x7J99Y",
      "authorship_tag": "ABX9TyPCiFN7oS9ZWyMfy1fXeq40",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Phonghuynh66/Artificial-Intelligence-Class/blob/master/Train_Handsign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtBMt1zacdT1"
      },
      "outputs": [],
      "source": [
        "# Huynh Quoc Phong 20149078\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import cv2\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import listdir\n",
        "from numpy import asarray, save\n",
        "from keras.utils import load_img, img_to_array\n",
        "from keras import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from PIL import Image\n",
        "from keras import models, layers, optimizers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image as image_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
      ],
      "metadata": {
        "id": "v96I_Nbde3VJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder='/content/drive/MyDrive/dataHandLang'\n",
        "photos,labels=list(),list()\n",
        "\n",
        "for file in listdir(folder):\n",
        "   if file.endswith('.jpg'):\n",
        "     output=0\n",
        "     if file.startswith('fist'):\n",
        "       output=1\n",
        "     if file.startswith('L'):\n",
        "       output=2\n",
        "     if file.startswith('okay'):\n",
        "       output=3\n",
        "     if file.startswith('palm'):\n",
        "       output=4\n",
        "     if file.startswith('peace'):\n",
        "       output=5\n",
        "     photo=load_img(folder+'/'+file,target_size=(224,224))\n",
        "     photo=img_to_array(photo)\n",
        "     photos.append(photo)\n",
        "     labels.append(output)\n",
        "\n",
        "photos=asarray(photos)\n",
        "labels=asarray(labels)\n",
        "\n",
        "print(photos.shape,labels.shape)\n",
        "\n",
        "save('flowers_photos.npy',photos)\n",
        "save('flowers_labels.npy',labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX8IgpBKNdJQ",
        "outputId": "43948194-4b7b-4e64-eda7-ef7e1b2a923f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2748, 224, 224, 3) (2748,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Xu ly du lieu dau vao\n",
        "def process_data(X_data, y_data):\n",
        "    X_data = np.array(X_data, dtype = 'float32')\n",
        "    if rgb:\n",
        "        pass\n",
        "    else:\n",
        "        X_data = np.stack((X_data,)*3, axis=-1)\n",
        "    X_data /= 255\n",
        "    y_data = np.array(y_data)\n",
        "    y_data = to_categorical(y_data)\n",
        "    return X_data, y_data"
      ],
      "metadata": {
        "id": "omrSlq9JR0BQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "lb=LabelBinarizer()\n",
        "labels=lb.fit_transform(labels)\n",
        "file=open('pix.data','wb')\n",
        "pickle.dump((photos,labels),file)\n",
        "file.close()\n",
        "\n",
        "def load_data():\n",
        "    file=open('pix.data','rb')\n",
        "    (photos, labels)=pickle.load(file)\n",
        "    file.close()\n",
        "    return photos, labels\n",
        "\n",
        "X,y=load_data()\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=100)\n",
        "\n",
        "print('X_train:',X_train.shape)\n",
        "print('X_test:',X_test.shape)\n",
        "print('y_train:',y_train.shape)\n",
        "print('y_test:',y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QtuTVkwSjfL",
        "outputId": "5f33f1b3-3b9f-46e3-e39d-a36654816251"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (2198, 224, 224, 3)\n",
            "X_test: (550, 224, 224, 3)\n",
            "y_train: (2198, 5)\n",
            "y_test: (550, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,Flatten,Conv2D,MaxPooling2D,LeakyReLU,Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras import losses\n",
        "   \n",
        "classes=5\n",
        "\n",
        "X_train=X_train.astype('float32')/255\n",
        "X_test=X_test.astype('float32')/255\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32,kernel_size=(3,3),activation='linear',input_shape=(224,224,3),padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(MaxPooling2D((2,2),padding='same'))\n",
        "\n",
        "model.add(Conv2D(64,kernel_size=(3,3),activation='linear',padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(MaxPooling2D((2,2),padding='same'))\n",
        "\n",
        "model.add(Conv2D(128,kernel_size=(3,3),activation='linear',padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(MaxPooling2D((2,2),padding='same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128,activation='linear'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(classes,activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwaanQSVTAk3",
        "outputId": "5d42abe1-7740-4621-9226-6a2fd4980b2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 224, 224, 32)      896       \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 224, 224, 32)      0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 112, 112, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 112, 112, 64)      18496     \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 56, 56, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 56, 56, 128)       73856     \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 28, 28, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 100352)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               12845184  \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,939,077\n",
            "Trainable params: 12,939,077\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "batch_size=64\n",
        "epochs=50   \n",
        "\n",
        "model.compile(loss=losses.categorical_crossentropy,optimizer=Adam(),metrics=['accuracy'])\n",
        "train=model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCvxw2FQW5xC",
        "outputId": "77590d9e-3d44-4f63-bb46-6261d7a51d85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "35/35 [==============================] - 15s 131ms/step - loss: 1.1639 - accuracy: 0.6028\n",
            "Epoch 2/50\n",
            "35/35 [==============================] - 4s 111ms/step - loss: 0.2441 - accuracy: 0.9163\n",
            "Epoch 3/50\n",
            "35/35 [==============================] - 4s 115ms/step - loss: 0.0757 - accuracy: 0.9786\n",
            "Epoch 4/50\n",
            "35/35 [==============================] - 4s 108ms/step - loss: 0.0347 - accuracy: 0.9882\n",
            "Epoch 5/50\n",
            "35/35 [==============================] - 4s 109ms/step - loss: 0.0082 - accuracy: 0.9986\n",
            "Epoch 6/50\n",
            "35/35 [==============================] - 4s 115ms/step - loss: 0.0118 - accuracy: 0.9955\n",
            "Epoch 7/50\n",
            "35/35 [==============================] - 4s 111ms/step - loss: 0.0419 - accuracy: 0.9895\n",
            "Epoch 8/50\n",
            "35/35 [==============================] - 4s 109ms/step - loss: 0.0127 - accuracy: 0.9950\n",
            "Epoch 9/50\n",
            "35/35 [==============================] - 4s 109ms/step - loss: 0.0028 - accuracy: 0.9991\n",
            "Epoch 10/50\n",
            "35/35 [==============================] - 4s 121ms/step - loss: 6.3458e-04 - accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "35/35 [==============================] - 4s 110ms/step - loss: 2.4603e-04 - accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "35/35 [==============================] - 4s 110ms/step - loss: 1.7139e-04 - accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "35/35 [==============================] - 4s 117ms/step - loss: 1.3597e-04 - accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "35/35 [==============================] - 4s 114ms/step - loss: 1.1291e-04 - accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "35/35 [==============================] - 4s 109ms/step - loss: 9.5395e-05 - accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "35/35 [==============================] - 4s 109ms/step - loss: 8.1958e-05 - accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "35/35 [==============================] - 4s 119ms/step - loss: 7.1565e-05 - accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "35/35 [==============================] - 4s 108ms/step - loss: 6.3147e-05 - accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "35/35 [==============================] - 4s 108ms/step - loss: 5.6132e-05 - accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "35/35 [==============================] - 4s 113ms/step - loss: 5.0131e-05 - accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "35/35 [==============================] - 4s 120ms/step - loss: 4.5261e-05 - accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "35/35 [==============================] - 4s 108ms/step - loss: 4.0922e-05 - accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "35/35 [==============================] - 4s 109ms/step - loss: 3.7480e-05 - accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "35/35 [==============================] - 4s 118ms/step - loss: 3.4310e-05 - accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "35/35 [==============================] - 4s 108ms/step - loss: 3.1615e-05 - accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "35/35 [==============================] - 4s 108ms/step - loss: 2.9162e-05 - accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "35/35 [==============================] - 4s 116ms/step - loss: 2.7058e-05 - accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "35/35 [==============================] - 4s 112ms/step - loss: 2.5065e-05 - accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "35/35 [==============================] - 4s 109ms/step - loss: 2.3364e-05 - accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "35/35 [==============================] - 4s 111ms/step - loss: 2.1820e-05 - accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "35/35 [==============================] - 4s 118ms/step - loss: 2.0409e-05 - accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "35/35 [==============================] - 4s 110ms/step - loss: 1.9133e-05 - accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "35/35 [==============================] - 4s 110ms/step - loss: 1.7988e-05 - accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "35/35 [==============================] - 4s 117ms/step - loss: 1.6934e-05 - accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "35/35 [==============================] - 4s 110ms/step - loss: 1.5953e-05 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "35/35 [==============================] - 4s 110ms/step - loss: 1.5043e-05 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "35/35 [==============================] - 4s 115ms/step - loss: 1.4230e-05 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "35/35 [==============================] - 4s 113ms/step - loss: 1.3470e-05 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "35/35 [==============================] - 4s 110ms/step - loss: 1.2769e-05 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "35/35 [==============================] - 4s 111ms/step - loss: 1.2126e-05 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "35/35 [==============================] - 4s 122ms/step - loss: 1.1525e-05 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "35/35 [==============================] - 4s 112ms/step - loss: 1.0953e-05 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "35/35 [==============================] - 4s 110ms/step - loss: 1.0431e-05 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "35/35 [==============================] - 4s 119ms/step - loss: 9.9344e-06 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "35/35 [==============================] - 4s 110ms/step - loss: 9.4671e-06 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "35/35 [==============================] - 4s 110ms/step - loss: 9.0394e-06 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "35/35 [==============================] - 4s 116ms/step - loss: 8.6203e-06 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "35/35 [==============================] - 4s 112ms/step - loss: 8.2427e-06 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "35/35 [==============================] - 4s 111ms/step - loss: 7.8854e-06 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "35/35 [==============================] - 4s 113ms/step - loss: 7.5407e-06 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/dataHandLang/handassign.h5\")\n"
      ],
      "metadata": {
        "id": "CiO2-YN5XXTd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "import time"
      ],
      "metadata": {
        "id": "uZIJokgzYsXQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_name = ['No/I Hate it','Note','Okay','Stop here','Hi/Hello']\n"
      ],
      "metadata": {
        "id": "w6Q4a1JTZNnH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('/content/drive/MyDrive/dataHandLang/handassign.h5')"
      ],
      "metadata": {
        "id": "QBnOf-uub_kf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ham du doan cu chi\n",
        "def predict_rgb_image_vgg(image):\n",
        "    image = np.array(image, dtype='float32')\n",
        "    image /= 255\n",
        "    pred_array = model.predict(image)\n",
        "    print(f'pred_array: {pred_array}')\n",
        "    result = class_name[np.argmax(pred_array)]\n",
        "    print(f'Result: {result}')\n",
        "    print(max(pred_array[0]))\n",
        "    score = float(\"%0.2f\" % (max(pred_array[0]) * 100))\n",
        "    print(result)\n",
        "    return result, score"
      ],
      "metadata": {
        "id": "KYINbuY0cIOx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ham xoa nen khoi anh\n",
        "def remove_background(frame):\n",
        "    fgmask = bgModel.apply(frame, learningRate=learningRate)\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    fgmask = cv2.erode(fgmask, kernel, iterations=1)\n",
        "    res = cv2.bitwise_and(frame, frame, mask=fgmask)\n",
        "    return res"
      ],
      "metadata": {
        "id": "N0okVHvncVvp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Khai bao kich thuoc vung detection region\n",
        "cap_region_x_begin = 0.5\n",
        "cap_region_y_end = 0.8\n",
        "\n",
        "# Cac thong so lay threshold\n",
        "threshold = 60\n",
        "blurValue = 41\n",
        "bgSubThreshold = 50#50\n",
        "learningRate = 0\n",
        "\n",
        "# Nguong du doan ky tu\n",
        "predThreshold= 95\n",
        "\n",
        "isBgCaptured = 0  # Bien luu tru da capture background chua\n"
      ],
      "metadata": {
        "id": "LNz1CnYtcZs7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Camera\n",
        "camera = cv2.VideoCapture(0)\n",
        "camera.set(10,200)\n",
        "camera.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.01)\n",
        "\n",
        "while camera.isOpened():\n",
        "    # Doc anh tu webcam\n",
        "    ret, frame = camera.read()\n",
        "    # Lam min anh\n",
        "    frame = cv2.bilateralFilter(frame, 5, 50, 100)\n",
        "    # Lat ngang anh\n",
        "    frame = cv2.flip(frame, 1)\n",
        "\n",
        "    # Ve khung hinh chu nhat vung detection region\n",
        "    cv2.rectangle(frame, (int(cap_region_x_begin * frame.shape[1]), 0),\n",
        "                  (frame.shape[1], int(cap_region_y_end * frame.shape[0])), (255, 0, 0), 2)\n",
        "\n",
        "    # Neu ca capture dc nen\n",
        "    if isBgCaptured == 1:\n",
        "        # Tach nen\n",
        "        img = remove_background(frame)\n",
        "\n",
        "        # Lay vung detection\n",
        "        img = img[0:int(cap_region_y_end * frame.shape[0]),\n",
        "              int(cap_region_x_begin * frame.shape[1]):frame.shape[1]]  # clip the ROI\n",
        "\n",
        "\n",
        "\n",
        "        # Chuyen ve den trang\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        blur = cv2.GaussianBlur(gray, (blurValue, blurValue), 0)\n",
        "\n",
        "        cv2.imshow('original1', cv2.resize(blur, dsize=None, fx=0.5, fy=0.5))\n",
        "\n",
        "        ret, thresh = cv2.threshold(blur, threshold, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "        cv2.imshow('thresh', cv2.resize(thresh, dsize=None, fx=0.5, fy=0.5))\n",
        "\n",
        "        if (np.count_nonzero(thresh)/(thresh.shape[0]*thresh.shape[0])>0.2):\n",
        "            # Neu nhu ve duoc hinh ban tay\n",
        "            if (thresh is not None):\n",
        "                # Dua vao mang de predict\n",
        "                target = np.stack((thresh,) * 3, axis=-1)\n",
        "                target = cv2.resize(target, (224, 224))\n",
        "                target = target.reshape(1, 224, 224, 3)\n",
        "                prediction, score = predict_rgb_image_vgg(target)\n",
        "\n",
        "                # Neu probality > nguong du doan thi hien thi\n",
        "                print(score,prediction)\n",
        "                if (score>=predThreshold):\n",
        "                    cv2.putText(frame, \"Sign:\" + prediction, (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 3,\n",
        "                                (0, 0, 255), 10, lineType=cv2.LINE_AA)\n",
        "    thresh = None\n",
        "\n",
        "    # Xu ly phim bam\n",
        "    k = cv2.waitKey(10)\n",
        "    if k == ord('q'):  # Bam q de thoat\n",
        "        break\n",
        "    elif k == ord('b'):\n",
        "        bgModel = cv2.createBackgroundSubtractorMOG2(0, bgSubThreshold)\n",
        "\n",
        "        isBgCaptured = 1\n",
        "        cv2.putText(frame, \"Background captured\", (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 3,\n",
        "                    (0, 0, 255), 10, lineType=cv2.LINE_AA)\n",
        "        time.sleep(2)\n",
        "        print('Background captured')\n",
        "\n",
        "    elif k == ord('r'):\n",
        "\n",
        "        bgModel = None\n",
        "        isBgCaptured = 0\n",
        "        cv2.putText(frame, \"Background reset\", (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 3,\n",
        "                    (0, 0, 255),10,lineType=cv2.LINE_AA)\n",
        "        print('Background reset')\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "    cv2.imshow('original', cv2.resize(frame, dsize=None, fx=0.5, fy=0.5))\n",
        "\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "camera.release()"
      ],
      "metadata": {
        "id": "PavcQi7AcgBK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1bVokonimrUN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}